# Saliency Fusion: Train a lightweight fusion network to combine multi-level cellular automata-enhanced saliency maps

This final step leverages the multi-level saliency maps generated by previous FLIM-CA pipeline stages to train lightweight neural networks that learn optimal fusion strategies. The approach addresses the challenge of combining saliency information from different encoder layers, improved by the cellular automata, into a single, refined output.

**Key Innovation**: Unlike simple averaging or maximum operations, our learned fusion approach adaptively weights contributions from each layer based on spatial context and content-specific patterns.

**Performance**: The lightweight fusion networks (typically <100K parameters) achieve improved segmentation performance while maintaining computational efficiency suitable for clinical deployment.

First, we need to install all necessary libraries through the code below:

```bash
python3.11 -m pip install torch==2.6.0 torchvision==0.21.0 monai==1.5.0 transformers
```

At this stage, we have multi-level saliency maps from different FLIM encoder layers (processed through cellular automata). The fusion network learns to combine these complementary representations to generate superior final predictions.


We have our auxiliary bash scripts, which enable us to train the merging model `learn_model_merging.sh` and `merge_saliencies.sh`. The first approach will iterate over each split, learning a model that merges saliencies from all learned models guided by the original input image. It calls the `train_merge_mode.py` with the following arguments:

1. Input folder with original images;
2. Input folder with the ground-truth labels;
3. Experiment folder (with the CA improved saliencies for all layers);
4. Path to save model weights;
5. Number of epochs to train;
6. Path to validation input images;
7. Path to validation ground-truth labels (Guides saving model weights as loss improves) --- if no path is provided, we use train loss.

> Validation: If you have downloaded the whole dataset, please configure the path to the validation folder.

Once the model is learned, the second auxiliary script (`merge_saliencies.sh`) can be employed to load the best learned model and run inferences on sample images. It calls the `run_inference.py` script with the following arguments:

1. Input folder with original images;
2. Experiment folder (with the CA improved saliencies for all layers);
3. Path to save model weights;
4. Output folder to save merged saliencies.

This step concludes our whole pipeline on sample images. We hope it can help and guide anyone who wants to test our pipeline on their data and under different regimes.

In the latter, we provide additional details about the merging model.

## Training Strategy

The fusion network training employs:

1. **Loss Function**: DiceFocalLoss, combining Dice loss for spatial overlap and Focal loss for hard example mining
2. **Regularization**: L1 penalty ($\lambda$=1e-3) to prevent overfitting
3. **Optimization**: Adam optimizer with cosine annealing schedule
4. **Data Augmentation**: Synchronized geometric and photometric transforms, including:
   - Random crops, rotations, and perspective transforms
   - Brightness/contrast adjustments and Gaussian blur
   - Horizontal/vertical flips

## Performance Considerations

- **Memory Efficiency**: Lightweight architectures suitable for resource-constrained environments
- **Training Time**: Typically converges within 1000-2000 epochs
- **Inference Speed**: Real-time capable on modern GPUs
- **Generalization**: Cross-validation across multiple splits ensures robust performance

The fusion approach represents the culmination of the FLIM-CA pipeline, transforming multi-level intermediate representations into final high-quality saliency predictions suitable for clinical applications.